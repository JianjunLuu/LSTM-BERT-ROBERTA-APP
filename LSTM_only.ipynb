{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8814959d",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce530031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear Principal,\\n\\nWe have been hearing quite ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dear principal hearing quite lot subject commu...</td>\n",
       "      <td>[dear, principal, hearing, quite, lot, subject...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dear [State Senator],\\n\\nI am writing to expr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>dear state senator writing express opinion ele...</td>\n",
       "      <td>[dear, state, senator, writing, express, opini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As high school students, we are constantly bom...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>high school students constantly bombarded info...</td>\n",
       "      <td>[high, school, students, constantly, bombarded...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi, I'm a 6th garden and I think zoos ANE Near...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>hi im 6th garden think zoos ane nearly cool iv...</td>\n",
       "      <td>[hi, im, 6th, garden, think, zoos, ane, nearly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sure, jar's my attempt at writing an essay as ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sure jars attempt writing essay average 8tj gr...</td>\n",
       "      <td>[sure, jars, attempt, writing, essay, average,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Good actions can be helpful in most ways. Good...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>good actions helpful ways good altered led goo...</td>\n",
       "      <td>[good, actions, helpful, ways, good, altered, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>The article \"Unmaking the Face on Mars\" explai...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>article unmaking face mars explains face mars ...</td>\n",
       "      <td>[article, unmaking, face, mars, explains, face...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Driving can be extremely dangerous for you or ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>driving extremely dangerous anyone else especi...</td>\n",
       "      <td>[driving, extremely, dangerous, anyone, else, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Hey there! \\n\\nSo, you know how people say \"ki...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>hey know people say kindness goes long way yea...</td>\n",
       "      <td>[hey, know, people, say, kindness, goes, long,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>I don't agree that we should Have driveless ca...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dont agree driveless cars lot things go wrong ...</td>\n",
       "      <td>[dont, agree, driveless, cars, lot, things, go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "0     Dear Principal,\\n\\nWe have been hearing quite ...    0.0   \n",
       "1      Dear [State Senator],\\n\\nI am writing to expr...    1.0   \n",
       "2     As high school students, we are constantly bom...    1.0   \n",
       "3     Hi, I'm a 6th garden and I think zoos ANE Near...    1.0   \n",
       "4     Sure, jar's my attempt at writing an essay as ...    1.0   \n",
       "...                                                 ...    ...   \n",
       "9995  Good actions can be helpful in most ways. Good...    0.0   \n",
       "9996  The article \"Unmaking the Face on Mars\" explai...    0.0   \n",
       "9997  Driving can be extremely dangerous for you or ...    0.0   \n",
       "9998  Hey there! \\n\\nSo, you know how people say \"ki...    1.0   \n",
       "9999  I don't agree that we should Have driveless ca...    0.0   \n",
       "\n",
       "                                                cleaned  \\\n",
       "0     dear principal hearing quite lot subject commu...   \n",
       "1     dear state senator writing express opinion ele...   \n",
       "2     high school students constantly bombarded info...   \n",
       "3     hi im 6th garden think zoos ane nearly cool iv...   \n",
       "4     sure jars attempt writing essay average 8tj gr...   \n",
       "...                                                 ...   \n",
       "9995  good actions helpful ways good altered led goo...   \n",
       "9996  article unmaking face mars explains face mars ...   \n",
       "9997  driving extremely dangerous anyone else especi...   \n",
       "9998  hey know people say kindness goes long way yea...   \n",
       "9999  dont agree driveless cars lot things go wrong ...   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [dear, principal, hearing, quite, lot, subject...  \n",
       "1     [dear, state, senator, writing, express, opini...  \n",
       "2     [high, school, students, constantly, bombarded...  \n",
       "3     [hi, im, 6th, garden, think, zoos, ane, nearly...  \n",
       "4     [sure, jars, attempt, writing, essay, average,...  \n",
       "...                                                 ...  \n",
       "9995  [good, actions, helpful, ways, good, altered, ...  \n",
       "9996  [article, unmaking, face, mars, explains, face...  \n",
       "9997  [driving, extremely, dangerous, anyone, else, ...  \n",
       "9998  [hey, know, people, say, kindness, goes, long,...  \n",
       "9999  [dont, agree, driveless, cars, lot, things, go...  \n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 数据读取\n",
    "data = pd.read_csv('C:/Users/ASUS/BERT/AI_Human.csv')\n",
    "\n",
    "# 数据采样与清洗\n",
    "ai_samples = data[data['generated'] == 1]\n",
    "human_samples = data[data['generated'] == 0]\n",
    "data = pd.concat([ai_samples.sample(n=5000, random_state=42), human_samples.sample(n=5000, random_state=42)])\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 清洗函数\n",
    "def remove_punc(text):\n",
    "    return ''.join([char for char in text if char not in punctuation])\n",
    "\n",
    "def remove_stop(text):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    return \" \".join([word for word in text.split() if word.lower() not in stops])\n",
    "\n",
    "# 文本清洗\n",
    "data['cleaned'] = data['text'].str.lower()\n",
    "data['cleaned'] = data['cleaned'].apply(lambda x: re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE))\n",
    "data['cleaned'] = data['cleaned'].apply(lambda x: re.sub(r'<.*?>', '', x))\n",
    "data['cleaned'] = data['cleaned'].apply(remove_punc)\n",
    "data['cleaned'] = data['cleaned'].apply(remove_stop)\n",
    "data['tokens'] = data['cleaned'].apply(word_tokenize)\n",
    "data.rename(columns={'generated': 'label'}, inplace=True)\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48225fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40157"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建词汇表\n",
    "vocab = Counter(word for tokens in data['tokens'] for word in tokens)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 创建词到索引的映射\n",
    "word_to_idx = {word: idx + 1 for idx, (word, _) in enumerate(vocab.most_common())}\n",
    "\n",
    "# 将文本转化为数字序列\n",
    "data['numerical'] = data['tokens'].apply(lambda x: [word_to_idx[word] for word in x if word in word_to_idx])\n",
    "\n",
    "# # 填充序列\n",
    "# max_len = 200\n",
    "# def truncate_and_pad(sequences, max_len):\n",
    "#     padded_sequences = []\n",
    "#     for seq in sequences:\n",
    "#         if len(seq) > max_len:  # 裁剪\n",
    "#             padded_sequences.append(seq[:max_len])\n",
    "#         else:  # 填充\n",
    "#             padded_sequences.append(seq + [0] * (max_len - len(seq)))\n",
    "#     return padded_sequences\n",
    "\n",
    "# X = truncate_and_pad(data['numerical'].values, max_len)\n",
    "# y = data['label'].values\n",
    "\n",
    "# # 数据划分\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=100)\n",
    "\n",
    "# # 转为 PyTorch 张量\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "# X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# # 保存数据处理后的数据\n",
    "# torch.save({\n",
    "#     'train_loader': train_loader,\n",
    "#     'test_loader': test_loader,\n",
    "#     'vocab_size': vocab_size,\n",
    "#     'word_to_idx': word_to_idx,  # 保存词到索引的映射\n",
    "# }, './processed_data.pt')\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3790b586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Certainly! Below is a reworded version of your...</td>\n",
       "      <td>1</td>\n",
       "      <td>certainly reworded version passage repetitive ...</td>\n",
       "      <td>[certainly, reworded, version, passage, repeti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>About twenty-five years ago, something happene...</td>\n",
       "      <td>0</td>\n",
       "      <td>twentyfive years ago something happened around...</td>\n",
       "      <td>[twentyfive, years, ago, something, happened, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Certainly! Here’s a reworded version with repe...</td>\n",
       "      <td>1</td>\n",
       "      <td>certainly here’s reworded version repetitive p...</td>\n",
       "      <td>[certainly, here, ’, s, reworded, version, rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dt is important for students to continue learn...</td>\n",
       "      <td>0</td>\n",
       "      <td>dt important students continue learning summer...</td>\n",
       "      <td>[dt, important, students, continue, learning, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In \"The Challenge of Exploring Venus\" the auth...</td>\n",
       "      <td>0</td>\n",
       "      <td>challenge exploring venus author suggests stud...</td>\n",
       "      <td>[challenge, exploring, venus, author, suggests...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>Certainly! Below is your passage reworded with...</td>\n",
       "      <td>1</td>\n",
       "      <td>certainly passage reworded repetitive phrases ...</td>\n",
       "      <td>[certainly, passage, reworded, repetitive, phr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>**A Good Attitude: The Key to Success**  \\n\\nA...</td>\n",
       "      <td>1</td>\n",
       "      <td>good attitude key success good attitude vital ...</td>\n",
       "      <td>[good, attitude, key, success, good, attitude,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>Certainly! Here’s a reworded version with repe...</td>\n",
       "      <td>1</td>\n",
       "      <td>certainly here’s reworded version repetitive p...</td>\n",
       "      <td>[certainly, here, ’, s, reworded, version, rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>Do You Know What The Electoral College Is?\\n\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>know electoral college electoral college place...</td>\n",
       "      <td>[know, electoral, college, electoral, college,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>I think it is a good idea that people should a...</td>\n",
       "      <td>0</td>\n",
       "      <td>think good idea people ask others peoples advi...</td>\n",
       "      <td>[think, good, idea, people, ask, others, peopl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "0     Certainly! Below is a reworded version of your...      1   \n",
       "1     About twenty-five years ago, something happene...      0   \n",
       "2     Certainly! Here’s a reworded version with repe...      1   \n",
       "3     Dt is important for students to continue learn...      0   \n",
       "4     In \"The Challenge of Exploring Venus\" the auth...      0   \n",
       "...                                                 ...    ...   \n",
       "3995  Certainly! Below is your passage reworded with...      1   \n",
       "3996  **A Good Attitude: The Key to Success**  \\n\\nA...      1   \n",
       "3997  Certainly! Here’s a reworded version with repe...      1   \n",
       "3998  Do You Know What The Electoral College Is?\\n\\n...      0   \n",
       "3999  I think it is a good idea that people should a...      0   \n",
       "\n",
       "                                                cleaned  \\\n",
       "0     certainly reworded version passage repetitive ...   \n",
       "1     twentyfive years ago something happened around...   \n",
       "2     certainly here’s reworded version repetitive p...   \n",
       "3     dt important students continue learning summer...   \n",
       "4     challenge exploring venus author suggests stud...   \n",
       "...                                                 ...   \n",
       "3995  certainly passage reworded repetitive phrases ...   \n",
       "3996  good attitude key success good attitude vital ...   \n",
       "3997  certainly here’s reworded version repetitive p...   \n",
       "3998  know electoral college electoral college place...   \n",
       "3999  think good idea people ask others peoples advi...   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [certainly, reworded, version, passage, repeti...  \n",
       "1     [twentyfive, years, ago, something, happened, ...  \n",
       "2     [certainly, here, ’, s, reworded, version, rep...  \n",
       "3     [dt, important, students, continue, learning, ...  \n",
       "4     [challenge, exploring, venus, author, suggests...  \n",
       "...                                                 ...  \n",
       "3995  [certainly, passage, reworded, repetitive, phr...  \n",
       "3996  [good, attitude, key, success, good, attitude,...  \n",
       "3997  [certainly, here, ’, s, reworded, version, rep...  \n",
       "3998  [know, electoral, college, electoral, college,...  \n",
       "3999  [think, good, idea, people, ask, others, peopl...  \n",
       "\n",
       "[4000 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据读取\n",
    "data = pd.read_csv('C:/Users/ASUS/BERT/data_Enhance/enhanceded_ai_human.csv')\n",
    "\n",
    "# 数据采样与清洗\n",
    "ai_samples = data[data['label'] == 1]\n",
    "human_samples = data[data['label'] == 0]\n",
    "data = pd.concat([ai_samples.sample(n=2000, random_state=42), human_samples.sample(n=2000, random_state=42)])\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 清洗函数\n",
    "def remove_punc(text):\n",
    "    return ''.join([char for char in text if char not in punctuation])\n",
    "\n",
    "def remove_stop(text):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    return \" \".join([word for word in text.split() if word.lower() not in stops])\n",
    "\n",
    "# 文本清洗\n",
    "data['cleaned'] = data['text'].str.lower()\n",
    "data['cleaned'] = data['cleaned'].apply(lambda x: re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE))\n",
    "data['cleaned'] = data['cleaned'].apply(lambda x: re.sub(r'<.*?>', '', x))\n",
    "data['cleaned'] = data['cleaned'].apply(remove_punc)\n",
    "data['cleaned'] = data['cleaned'].apply(remove_stop)\n",
    "data['tokens'] = data['cleaned'].apply(word_tokenize)\n",
    "\n",
    "data_aug=data\n",
    "data_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31e059f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26503"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建词汇表\n",
    "vocab = Counter(word for tokens in data_aug['tokens'] for word in tokens)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 创建词到索引的映射\n",
    "word_to_idx = {word: idx + 1 for idx, (word, _) in enumerate(vocab.most_common())}\n",
    "\n",
    "# 将文本转化为数字序列\n",
    "data_aug['numerical'] = data_aug['tokens'].apply(lambda x: [word_to_idx[word] for word in x if word in word_to_idx])\n",
    "\n",
    "# # 填充序列\n",
    "# max_len = 200\n",
    "# def truncate_and_pad(sequences, max_len):\n",
    "#     padded_sequences = []\n",
    "#     for seq in sequences:\n",
    "#         if len(seq) > max_len:  # 裁剪\n",
    "#             padded_sequences.append(seq[:max_len])\n",
    "#         else:  # 填充\n",
    "#             padded_sequences.append(seq + [0] * (max_len - len(seq)))\n",
    "#     return padded_sequences\n",
    "\n",
    "# X = truncate_and_pad(data_aug['numerical'].values, max_len)\n",
    "# y = data_aug['label'].values\n",
    "\n",
    "# # 数据划分\n",
    "# X_train, _, y_train, _ = train_test_split(X, y, test_size=0.1, random_state=100)\n",
    "\n",
    "# # 转为 PyTorch 张量\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "# train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# # 保存数据处理后的数据\n",
    "# torch.save({\n",
    "#     'train_loader_aug': train_loader,\n",
    "#     'test_loader_aug': test_loader,\n",
    "#     'vocab_size_aug': vocab_size,\n",
    "#     'word_to_idx_aug': word_to_idx,  # 保存词到索引的映射\n",
    "# }, './processed_data_aug.pt')\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afe0c450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26504"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 假设 data_original 和 data_augmented 都已经有了 `tokens` 列\n",
    "all_tokens = list(data['tokens']) + list(data_aug['tokens'])\n",
    "\n",
    "# 构建词表\n",
    "vocab = Counter(word for tokens in all_tokens for word in tokens)\n",
    "\n",
    "# 建立 word -> index 映射\n",
    "word_to_idx = {word: idx + 1 for idx, (word, _) in enumerate(vocab.most_common())}\n",
    "vocab_size = len(word_to_idx) + 1  # +1 for padding index 0\n",
    "\n",
    "data['numerical'] = data['tokens'].apply(lambda x: [word_to_idx.get(word, 0) for word in x])\n",
    "data_aug['numerical'] = data_aug['tokens'].apply(lambda x: [word_to_idx.get(word, 0) for word in x])\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ae0a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充序列\n",
    "max_len = 200\n",
    "def truncate_and_pad(sequences, max_len):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > max_len:  # 裁剪\n",
    "            padded_sequences.append(seq[:max_len])\n",
    "        else:  # 填充\n",
    "            padded_sequences.append(seq + [0] * (max_len - len(seq)))\n",
    "    return padded_sequences\n",
    "\n",
    "X = truncate_and_pad(data['numerical'].values, max_len)\n",
    "y = data['label'].values\n",
    "\n",
    "# 数据划分\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=100)\n",
    "\n",
    "# 转为 PyTorch 张量\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 保存数据处理后的数据\n",
    "torch.save({\n",
    "    'train_loader': train_loader,\n",
    "    'test_loader': test_loader,\n",
    "    'vocab_size': vocab_size,\n",
    "    'word_to_idx': word_to_idx,  # 保存词到索引的映射\n",
    "}, './processed_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bd9886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充序列\n",
    "max_len = 200\n",
    "def truncate_and_pad(sequences, max_len):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > max_len:  # 裁剪\n",
    "            padded_sequences.append(seq[:max_len])\n",
    "        else:  # 填充\n",
    "            padded_sequences.append(seq + [0] * (max_len - len(seq)))\n",
    "    return padded_sequences\n",
    "\n",
    "X = truncate_and_pad(data_aug['numerical'].values, max_len)\n",
    "y = data_aug['label'].values\n",
    "\n",
    "# 数据划分\n",
    "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.1, random_state=100)\n",
    "\n",
    "# 转为 PyTorch 张量\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 保存数据处理后的数据\n",
    "torch.save({\n",
    "    'train_loader_aug': train_loader,\n",
    "    'test_loader_aug': test_loader,\n",
    "    'vocab_size_aug': vocab_size,\n",
    "    'word_to_idx_aug': word_to_idx,  # 保存词到索引的映射\n",
    "}, './processed_data_aug.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b024db86",
   "metadata": {},
   "source": [
    "## 原始lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "026015a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21612\\2319843758.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load('./processed_data.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 55.7771, Accuracy: 0.7675\n",
      "Epoch 2/5, Loss: 31.6702, Accuracy: 0.8739\n",
      "Epoch 3/5, Loss: 27.4782, Accuracy: 0.8928\n",
      "Epoch 4/5, Loss: 31.0279, Accuracy: 0.8917\n",
      "Epoch 5/5, Loss: 43.9367, Accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# 加载数据处理后的数据\n",
    "data = torch.load('./processed_data.pt')\n",
    "train_loader = data['train_loader']\n",
    "test_loader = data['test_loader']\n",
    "vocab_size = data['vocab_size']#每个文本的单词数量（固定好了），每个单词对应一个索引\n",
    "\n",
    "# 定义 LSTM 模型\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)#输入索引，输出向量，每个索引对应一个embedding_dim大小的向量\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]  # 取最后时间步的输出\n",
    "        x = self.fc(self.dropout(lstm_out))\n",
    "        return x\n",
    "\n",
    "# 模型参数\n",
    "embedding_dim = 128\n",
    "hidden_dim = 128\n",
    "output_dim = 2\n",
    "dropout_rate = 0\n",
    "\n",
    "model = LSTMClassifier(vocab_size + 1, embedding_dim, hidden_dim, output_dim, dropout_rate)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 模型训练\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    train_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), './lstm_model_orig.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965b9924",
   "metadata": {},
   "source": [
    "## 增强LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3622c294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21612\\2915289651.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load('./processed_data_aug.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 56.7731, Accuracy: 0.7900\n",
      "Epoch 2/5, Loss: 22.7473, Accuracy: 0.9419\n",
      "Epoch 3/5, Loss: 21.2092, Accuracy: 0.9469\n",
      "Epoch 4/5, Loss: 21.6655, Accuracy: 0.9422\n",
      "Epoch 5/5, Loss: 47.1287, Accuracy: 0.8078\n"
     ]
    }
   ],
   "source": [
    "# 加载数据处理后的数据\n",
    "data = torch.load('./processed_data_aug.pt')\n",
    "train_loader = data['train_loader_aug']\n",
    "test_loader = data['test_loader_aug']\n",
    "vocab_size = data['vocab_size_aug']#每个文本的单词数量（固定好了），每个单词对应一个索引\n",
    "\n",
    "# 定义 LSTM 模型\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)#输入索引，输出向量，每个索引对应一个embedding_dim大小的向量\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]  # 取最后时间步的输出\n",
    "        x = self.fc(self.dropout(lstm_out))\n",
    "        return x\n",
    "\n",
    "# 模型参数\n",
    "embedding_dim = 128\n",
    "hidden_dim = 128\n",
    "output_dim = 2\n",
    "dropout_rate = 0\n",
    "\n",
    "model = LSTMClassifier(vocab_size + 1, embedding_dim, hidden_dim, output_dim, dropout_rate)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 模型训练\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    train_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), './lstm_model_aug.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9734f6",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7dd3bf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21612\\1958019421.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_orig = torch.load('./processed_data.pt')\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21612\\1958019421.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_aug = torch.load('./processed_data_aug.pt')\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21612\\1958019421.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加权融合准确率：0.8250\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 模型结构（和之前一致）\n",
    "# 定义 LSTM 模型\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)#输入索引，输出向量，每个索引对应一个embedding_dim大小的向量\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]  # 取最后时间步的输出\n",
    "        x = self.fc(self.dropout(lstm_out))\n",
    "        return x\n",
    "\n",
    "\n",
    "# 加载处理数据\n",
    "data_orig = torch.load('./processed_data.pt')\n",
    "data_aug = torch.load('./processed_data_aug.pt')\n",
    "\n",
    "vocab_size_orig = data_orig['vocab_size']\n",
    "vocab_size_aug = data_aug['vocab_size_aug']\n",
    "test_loader = data_orig['test_loader']  # 使用原始测试集\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# 加载模型函数\n",
    "def load_model(path, vocab_size):\n",
    "    model = LSTMClassifier(vocab_size + 1, 128, 128, 2, 0.3)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model_orig = load_model('./lstm_model_orig.pth', vocab_size_orig)\n",
    "model_aug = load_model('./lstm_model_aug.pth', vocab_size_aug)\n",
    "\n",
    "# 融合预测函数\n",
    "def evaluate_weighted_ensemble(model1, model2, test_loader, alpha):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            out1 = F.softmax(model1(X_batch), dim=1)\n",
    "            out2 = F.softmax(model2(X_batch), dim=1)\n",
    "\n",
    "            fused = alpha * out1 + (1 - alpha) * out2\n",
    "            _, predicted = torch.max(fused, 1)\n",
    "\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# 输出准确率\n",
    "acc_fused = evaluate_weighted_ensemble(model_orig, model_aug, test_loader, alpha=0.5)\n",
    "\n",
    "print(f\"加权融合准确率：{acc_fused:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a121f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
