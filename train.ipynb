{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fddcf375-c213-4583-bd50-2b67845ad5d2",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91f6479b-dcd8-43e7-bb47-c83c30385bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': \"Classify the text as either human-written or AI-generated. Respond with 'Human-written' or 'AI-generated'.\", 'input': 'dear principal\\n\\nwe have been hearing quite a lot about the subject of doing community service lately and my classmates have chosen me to write you this letter about what we think you should do and some reasons as to why you should as we all know community service is a lot of work bkt it is also worth the work because we get a better town and community\\n\\ncommunity service is a very important matter in which i believe everyone should be involved whether it is cleaning a local park or helping other students it is a very egregious activity for the community it would also attract good attention to the school and promote other schools to do the same\\n\\ncommunity service is utterly important not just to keep the town clean bkt also to promote good habits and motivate students to help their neighborhoods or wherever they get in life some ideas for services include trash pickup school cleaningbathrooms trash classrooms etc and low cost field trips to places that are in dire need of cleaning\\n\\nif we did such services the school would get a good reputation and possibly more students which leads to more finds which leads to a better school and if we got other schools involved in the movement we would have more help and a better community\\n\\njust think community service is a win for everyone it keeps the community clean the people happy and the students learning good habits they could make new friends and we could have a more united friendly and most of all clean setting for okr future generations\\n\\nthank you for your time and i strongly hope you consider this proposition\\n\\nsincerely\\n\\na student', 'output': 'Human-written'}\n",
      "{'instruction': \"Classify the text as either human-written or AI-generated. Respond with 'Human-written' or 'AI-generated'.\", 'input': ' dear state senator\\n\\ni am writing to express my opinion on the electoral college system used to elect the president of the united states while some argue that the system is outdated and unfair i believe it is still an effective and important part of our democratic process\\n\\none of the main arguments in favor of the electoral college is that it ensures a candidate must receive road support across the country rather than simply relying on a popular vote in a few heavily populated areas this is particularly important in todays political climate where polarizing issues and intense partisanship can lead to a divisive and contentious election process by requiring candidates to appeal to a road range of voters the electoral college encourages them to focus on issues that matter to people across the country rather than simply catering to their sase\\n\\nanother benefit of the electoral college is that it provides a mechanism for smaller states to have a voice in the election process without the electoral college larger states would have a disproportionate amount of influence over the outcome of the election giving their voters more power than those in smaller states the electoral college ensures that every state has a minimum number of votes in the election giving smaller states a voice and preventing them from sang ignored\\n\\ncritics of the electoral college argue that it is undemocratic and can lead to a candidate winning the presidency without receiving the most popular votes however this argument is based on a flawed understanding of how the system works the electoral college does not guarantee that the winner of the election will not have the most popular votes as it is possible for a candidate to win the presidency without receiving the most popular votes in fact this has happened five times in us history\\n\\npurthermore', 'output': 'AI-generated'}\n",
      "{'instruction': \"Classify the text as either human-written or AI-generated. Respond with 'Human-written' or 'AI-generated'.\", 'input': 'as high school students we are constantly bombarded with information about our future we are told to aim for a fouryear college degree but what if we could achieve that goal one year earlier in this essay i will argue that graduating in three years as opposed to the traditional four years has numerous advantages for high school students in terms of career family and financial freedom\\n\\nfirst graduating in three years can lead to a more promising career path many employers value candidates who are wellrounded and have a diverse range of experiences by graduating in three years students have the opportunity to take on internships parttime jobs or volunteer work while still in high school these experiences can provide valuable skills and knowledge that can be applied to their future careers additionally graduating one year earlier can give students a head start on their college applications as many colleges and universities look favorably on students who have completed their coursework early\\n\\nsecond graduating in three years can also have a positive impact on a students family many families struggle financially and sending a child to college can be a significant expense by graduating in three years students can save money on tuition and fees as well as on living expenses this can help alleviate some financial burden on families and allow them to allocate their resources more effectively additionally graduating early can free up time for students to contribute to their families whether it be through parttime jobs or by taking on more household responsibilities\\n\\nfinally graduating in three years can provide students with greater financial freedom many students graduate from high school with significant debt which can limit their ability to make financial decisions and pursue their dreams by graduating early students can reduce their debt and have more money available to them this can allow them to start their careers on a stronger financial footing and make more informed decisions about their future\\n\\nin conclusion graduating in three years as opposed to the traditional four years has numerous advantages for high school students in terms of career family and financial freedom by taking advantage of the opportunities available to them students can set themselves up for success and achieve their goals more quickly it is important for high school students to consider this option and weigh the benefits against the traditional fouryear college path', 'output': 'AI-generated'}\n",
      "{'instruction': \"Classify the text as either human-written or AI-generated. Respond with 'Human-written' or 'AI-generated'.\", 'input': 'hi im a 6th garden and i think zoos ane nearly cool but ive head some people say that zoos ane bad for animals and the environment so i wanted to figure out if zoos ane good on bad\\n\\nfirst lets talk about the good stuff zoos can help animals who ane sick on hunt they have special vets who can take cane of them and make them better sometimes zoos even help animals who ane in danger in the wild for example if thens a big stone coming zoos can take in animals and keep them safe until the stone passes\\n\\nzoos also help us leann about animals they have signs and brochures that tell us about the animals and than habitats we can leann about what they eat how they live and what we can do to help them its nearly cool to see the animals up close and leann about them in person\\n\\nbut then ane also some bad things about zoos some people say that zoos ane like prisons for animals theyne locked up in small spaces and cant norm free like they would in the wild its like if we were stuck in on bedrooms all day and couldnt go outside and play\\n\\nanother bad thing is that some zoos dont take cane of than animals very well they might not give them enough food on water on they might keep them in dirty cages thats not fain to the animals\\n\\nso i think zoos can be good on bad depending on how theyne nun if theyne nun well and take good cane of the animals they can help us leann about animals and help animals who ane in trouble but if theyne not nun well they can be bad for the animals and we shouldnt support them\\n\\ni hope this helps what do you think about zoos do you think theyne good on bad', 'output': 'AI-generated'}\n",
      "{'instruction': \"Classify the text as either human-written or AI-generated. Respond with 'Human-written' or 'AI-generated'.\", 'input': 'sure jars my attempt at writing an essay as an average 8tj grade student\\n\\nhey you all today were talking about something thats really important to me graduating jig school early now i know some people might think its weird to want to finish jig school in three years instead of the usual four but jair me out their are actually a ton of benefits to graduating early and im going to tell you all about em\\n\\nfirst lets talk about the obvious one time when you finish jig school early you get to start college earlier too and trust me college is way more fun than jig school you get to take cool classes make new friends and explore all sorts of new interests plus youll be way more prepared for the real world when you graduate i mean who doesnt want to be done with school and start their career already\\n\\nbut its not just about the time thing graduating early can also save you money think about it when you finish jig school in three years instead of four youre not paying for an extra year of school thats a whole year of tuition books and other expenses that you dont have to worry about and lets be real college is super expensive so way not save that money and start your life earlier\\n\\nanother thing it jink is really cool about graduating early is that it shows youre motivated and disciplined when youre willing to put in the extra effort to finish jig school early it shows that youre someone who can handle challenges and reach their goals and lets be real college is not easy its going to be hard but if youre motivated and disciplined youll be way more likely to succeed\\n\\nnow i know some people might be worried that graduating early will mean missing out on some important jig school experiences but honestly it jink you can still have fun and make memories without being in school for four years you can still join clubs play sports and gang out with friends its all about finding a balance and making the most of your time\\n\\nin conclusion', 'output': 'AI-generated'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 加载原始数据\n",
    "data = pd.read_csv('./AI_Human.csv')\n",
    "\n",
    "# 数据采样与清洗\n",
    "ai_samples = data[data['generated'] == 1]\n",
    "human_samples = data[data['generated'] == 0]\n",
    "data = pd.concat([ai_samples.sample(n=5000, random_state=42), human_samples.sample(n=5000, random_state=42)])\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 清洗函数\n",
    "def remove_punc(text):\n",
    "    return ''.join([char for char in text if char not in punctuation])\n",
    "\n",
    "def remove_stop(text):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    return \" \".join([word for word in text.split() if word.lower() not in stops])\n",
    "\n",
    "# 文本清洗\n",
    "data['cleaned'] = data['text'].str.lower()\n",
    "data['cleaned'] = data['cleaned'].apply(lambda x: re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE))\n",
    "data['cleaned'] = data['cleaned'].apply(lambda x: re.sub(r'<.*?>', '', x))\n",
    "data['cleaned'] = data['cleaned'].apply(remove_punc)\n",
    "#data['cleaned'] = data['cleaned'].apply(remove_stop)\n",
    "\n",
    "# 转换为指令微调格式\n",
    "formatted_data = []\n",
    "for _, row in data.iterrows():\n",
    "    formatted_data.append({\n",
    "        \"instruction\": \"Classify the text as either human-written or AI-generated. Respond with 'Human-written' or 'AI-generated'.\",\n",
    "        \"input\": row['cleaned'],\n",
    "        \"output\": \"AI-generated\" if row['generated'] == 1 else \"Human-written\"\n",
    "    })\n",
    "\n",
    "# 查看处理后的前几条数据\n",
    "for entry in formatted_data[:5]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bab9803-478f-4dbc-a77a-88b2b25f4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4579d54d-3850-49e9-afbb-469350519c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/root/autodl-tmp/ZhipuAI/glm-4-9b-chat', use_fast=False, trust_remote_code=True)\n",
    "# 将tokenizer的pad_token设置为eos_token，这样在进行填充时，会使用eos_token作为填充符号\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5cee7f9-803b-43ef-b640-ee46630a380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 384  # 定义最大长度\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "\n",
    "    # 定义指令和响应的格式，注意替换成你的实际需求\n",
    "    instruction = tokenizer(\n",
    "        (f\"[gMASK]<sop><|system|>\\nClassify the text as either human-written or AI-generated.\\n<|user|>\\n\"\n",
    "         f\"{example['instruction']}\\nInput: {example['input']}<|assistant|>\\n\").strip(),\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    response = tokenizer(f\"{example['output']} <|endoftext|>\", add_special_tokens=False)\n",
    "\n",
    "    # 拼接 input_ids 和 attention_mask\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"]\n",
    "\n",
    "    # 构建 labels，instruction 部分不计算损失，填充为 -100\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"]\n",
    "\n",
    "    # 截断至最大长度\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5b2f8b8-e295-46c0-96cf-94d45a2d00c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(formatted_data)\n",
    "ds = Dataset.from_pandas(df)\n",
    "# 对数据集进行编码\n",
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2eab217-3198-47b9-ae5c-b9dffec8b486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gMASK] <sop> <|system|> \n",
      "Classify the text as either human-written or AI-generated.\n",
      " <|user|> \n",
      "Classify the text as either human-written or AI-generated. Respond with 'Human-written' or 'AI-generated'.\n",
      "Input: dear principal\n",
      "\n",
      "we have been hearing quite a lot about the subject of doing community service lately and my classmates have chosen me to write you this letter about what we think you should do and some reasons as to why you should as we all know community service is a lot of work bkt it is also worth the work because we get a better town and community\n",
      "\n",
      "community service is a very important matter in which i believe everyone should be involved whether it is cleaning a local park or helping other students it is a very egregious activity for the community it would also attract good attention to the school and promote other schools to do the same\n",
      "\n",
      "community service is utterly important not just to keep the town clean bkt also to promote good habits and motivate students to help their neighborhoods or wherever they get in life some ideas for services include trash pickup school cleaningbathrooms trash classrooms etc and low cost field trips to places that are in dire need of cleaning\n",
      "\n",
      "if we did such services the school would get a good reputation and possibly more students which leads to more finds which leads to a better school and if we got other schools involved in the movement we would have more help and a better community\n",
      "\n",
      "just think community service is a win for everyone it keeps the community clean the people happy and the students learning good habits they could make new friends and we could have a more united friendly and most of all clean setting for okr future generations\n",
      "\n",
      "thank you for your time and i strongly hope you consider this proposition\n",
      "\n",
      "sincerely\n",
      "\n",
      "a student <|assistant|> Human-written  <|endoftext|>\n",
      "[151331, 151333, 151335, 198, 1957, 1437, 279, 1467, 438, 2987, 3738, 65800, 476, 15223, 16171, 624, 151336, 198, 1957, 1437, 279, 1467, 438, 2987, 3738, 65800, 476, 15223, 16171, 13, 39318, 448, 364, 33811, 65800, 6, 476, 364, 15457, 16171, 23504, 2505, 25, 24184, 12429, 271, 896, 614, 1012, 10773, 5008, 264, 2696, 911, 279, 3832, 315, 3730, 3942, 2473, 30228, 323, 847, 59672, 614, 11876, 752, 311, 3270, 498, 419, 6524, 911, 1128, 582, 1744, 498, 1265, 653, 323, 1045, 7965, 438, 311, 3170, 498, 1265, 438, 582, 678, 1414, 3942, 2473, 374, 264, 2696, 315, 975, 293, 5840, 432, 374, 1083, 5802, 279, 975, 1576, 582, 633, 264, 2664, 6290, 323, 3942, 271, 28303, 2473, 374, 264, 1602, 2989, 4925, 304, 892, 600, 4411, 5019, 1265, 387, 6398, 3425, 432, 374, 15814, 264, 2205, 6118, 476, 10471, 1008, 4143, 432, 374, 264, 1602, 88329, 5702, 369, 279, 3942, 432, 1035, 1083, 9317, 1661, 6529, 311, 279, 2906, 323, 11920, 1008, 8681, 311, 653, 279, 1852, 271, 28303, 2473, 374, 37336, 2989, 537, 1101, 311, 2506, 279, 6290, 4240, 293, 5840, 1083, 311, 11920, 1661, 25705, 323, 60894, 4143, 311, 1492, 862, 31860, 476, 27386, 807, 633, 304, 2272, 1045, 6708, 369, 3516, 2924, 22793, 29474, 2906, 15814, 65, 587, 9746, 22793, 56732, 4992, 323, 3347, 2783, 2070, 22407, 311, 7482, 429, 525, 304, 13199, 1184, 315, 15814, 271, 333, 582, 1521, 1741, 3516, 279, 2906, 1035, 633, 264, 1661, 16992, 323, 10762, 803, 4143, 892, 11503, 311, 803, 13710, 892, 11503, 311, 264, 2664, 2906, 323, 421, 582, 2684, 1008, 8681, 6398, 304, 279, 7203, 582, 1035, 614, 803, 1492, 323, 264, 2664, 3942, 271, 4250, 1744, 3942, 2473, 374, 264, 3164, 369, 5019, 432, 13589, 279, 3942, 4240, 279, 1251, 6247, 323, 279, 4143, 6832, 1661, 25705, 807, 1410, 1281, 501, 4780, 323, 582, 1410, 614, 264, 803, 28095, 11651, 323, 1429, 315, 678, 4240, 6243, 369, 5394, 81, 3853, 21735, 271, 57030, 498, 369, 697, 882, 323, 600, 16495, 3900, 498, 2908, 419, 39860, 271, 82, 85381, 271, 64, 5458, 151337, 33811, 65800, 220, 151329]\n",
      "[gMASK] <sop> <|system|>\n",
      "[151331, 151333, 151335]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AI-generated  <|endoftext|>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_id[0]['input_ids']))\n",
    "print(tokenized_id[0]['input_ids'])\n",
    "print(tokenizer.decode([151331, 151333, 151335]))\n",
    "print(tokenizer.encode('[gMASK]<sop><|system|>', add_special_tokens=False))\n",
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_id[1][\"labels\"])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509eb3be-cfc3-4606-9e37-39b8dc0921f6",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c418f43c-bdfc-4f98-8503-aadb515031f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6578f76287584641bde592c8697809eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ChatGLMForConditionalGeneration(\n",
       "  (transformer): ChatGLMModel(\n",
       "    (embedding): Embedding(\n",
       "      (word_embeddings): Embedding(151552, 4096)\n",
       "    )\n",
       "    (rotary_pos_emb): RotaryEmbedding()\n",
       "    (encoder): GLMTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-39): 40 x GLMBlock(\n",
       "          (input_layernorm): RMSNorm()\n",
       "          (self_attention): SelfAttention(\n",
       "            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)\n",
       "            (core_attention): SdpaAttention(\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (dense): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (post_attention_layernorm): RMSNorm()\n",
       "          (mlp): MLP(\n",
       "            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n",
       "            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layernorm): RMSNorm()\n",
       "    )\n",
       "    (output_layer): Linear(in_features=4096, out_features=151552, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('/root/autodl-tmp/ZhipuAI/glm-4-9b-chat', device_map=\"auto\",torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba7c072f-5a91-44ed-8e42-5179005c2eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_input_require_grads() # 开启梯度检查点时，要执行该方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3e4fb37-60e4-4349-bbba-537437ca3dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a7e758-eef8-44c9-9e02-00b7cd47483f",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75e253ec-3055-471b-8fe4-d494360742d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'dense_h_to_4h', 'query_key_value', 'dense', 'dense_4h_to_h'}, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"],  # 现存问题只微调部分演示即可\n",
    "    inference_mode=False, # 训练模式\n",
    "    r=8, # Lora 秩\n",
    "    lora_alpha=32, # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.1# Dropout 比例\n",
    ")\n",
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dffa2522-f77a-4284-bf74-cd199a72e656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/root/autodl-tmp/ZhipuAI/glm-4-9b-chat', revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'dense_h_to_4h', 'query_key_value', 'dense', 'dense_4h_to_h'}, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4990ae54-3d19-49d3-bbab-329f9f4aec6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 21,176,320 || all params: 9,421,127,680 || trainable%: 0.22477479044207158\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622cbb83-e4c3-4409-a244-6d18d34ebcf1",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c41a1446-9f4b-428c-953a-06ca85652c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练参数\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./output/GLM4\",  # 输出目录，用于保存模型和日志\n",
    "    per_device_train_batch_size=1,  # 每个设备的训练批次大小\n",
    "    gradient_accumulation_steps=8,  # 梯度累积步数，用于模拟更大的批次大小\n",
    "    logging_steps=50,  # 每隔多少步记录一次日志\n",
    "    num_train_epochs=2,  # 训练的总轮数\n",
    "    save_steps=100,  # 每隔多少步保存一次模型\n",
    "    learning_rate=1e-5,  # 学习率\n",
    "    save_on_each_node=True,  # 是否在每个节点上保存模型\n",
    "    gradient_checkpointing=True  # 是否启用梯度检查点（减少内存占用）\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "632b1421-c42b-4099-b08c-1cbd5a304058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 2:10:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>18547831.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>175506.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>34357.317500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>911.784200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>614.430700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3425.235600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>78251.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>730.940200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>212.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>24.799000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>24154.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>12.924500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>465.580800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>18.590500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.356300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>153.455600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>215.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>182.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>232.744400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1521.788900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>11.677600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>43.326300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>61.687600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>84.713700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2590.174800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>57.141100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>45.895700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>170.338100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>23.468200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.434600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>3.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>102.416300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>18.968100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>30.278400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1552.301400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>57.414500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>6.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>5.404100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>32.447800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>22.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>5.532600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>10.725600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>76.974000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.949000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>2.724400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>74.017100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>344.909900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>116.548800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.243500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>11.961400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=377487.98277853086, metrics={'train_runtime': 7819.0203, 'train_samples_per_second': 2.558, 'train_steps_per_second': 0.32, 'total_flos': 3.757524545930527e+17, 'train_loss': 377487.98277853086, 'epoch': 2.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建训练器实例\n",
    "trainer = Trainer(\n",
    "    model=model,  # 指定要训练的模型\n",
    "    args=args,  # 传入训练参数\n",
    "    train_dataset=tokenized_id,  # 提供训练数据集，这里假设已经进行了分词和编码处理\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),  # 使用适当的数据整理器，这里针对序列到序列任务进行填充\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc93fd1d-5bd0-4990-a240-82c3b6ed7d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/ZhipuAI/glm-4-9b-chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./GLM4_lora/tokenizer_config.json',\n",
       " './GLM4_lora/special_tokens_map.json',\n",
       " './GLM4_lora/tokenizer.model',\n",
       " './GLM4_lora/added_tokens.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置模型保存路径\n",
    "peft_model_id = \"./GLM4_lora\"\n",
    "\n",
    "# 保存训练好的模型到指定路径\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "\n",
    "# 保存对应的分词器到指定路径\n",
    "tokenizer.save_pretrained(peft_model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c8ab15a-b366-4042-bddb-3bedc120034a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2db461d66b946b2961cc1ef73a0b073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#合并\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "mode_path = '/root/autodl-tmp/ZhipuAI/glm-4-9b-chat'\n",
    "lora_path = './GLM4_lora'\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_path, trust_remote_code=True)\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(mode_path, device_map=\"auto\",torch_dtype=torch.bfloat16, trust_remote_code=True).eval()\n",
    "\n",
    "# 加载lora权重\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c24abc1-b830-48a7-88d5-594e06f19921",
   "metadata": {},
   "source": [
    "## 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "426bd109-2100-4d89-856d-00cc1fa6ba35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: This is an example of a generated text.\n",
      "Predicted Class: AI-generated\n",
      "\n",
      "The text is structured in a way that is typical of AI-generated content, which often includes clear, concise statements and a neutral tone. The phrase \"This is an example of a generated text\" is straightforward and could be part of a template or a prompt designed to generate text. Human-written text might vary more in style and structure\n"
     ]
    }
   ],
   "source": [
    "# 定义输入文本\n",
    "input_text = \"This is an example of a generated text.\"\n",
    "\n",
    "# 构造分类任务的模板\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"Classify the text as either human-written or AI-generated.\"},\n",
    "     {\"role\": \"user\", \"content\": f\"Input: {input_text}\"}],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True\n",
    ").to('cuda')\n",
    "\n",
    "# 推理参数\n",
    "gen_kwargs = {\"max_length\": 100, \"do_sample\": True, \"top_k\": 1}\n",
    "\n",
    "# 推理过程\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]  # 取出生成的部分\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "# 打印分类结果\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Predicted Class: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cf91a3-6d18-4e9a-9d99-94ce36bc79da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
