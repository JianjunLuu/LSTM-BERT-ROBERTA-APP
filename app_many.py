import streamlit as st
import torch
import torch.nn as nn
import torch.nn.functional as F
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from string import punctuation
import re
import numpy as np
import pickle
from transformers import BertForSequenceClassification, BertTokenizer, RobertaForSequenceClassification, RobertaTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from xgboost import XGBClassifier
from openai import OpenAI

# ç¡®ä¿ä¸‹è½½å¿…è¦çš„ NLTK èµ„æº
def ensure_resources_downloaded():
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
    try:
        stopwords.words('english')
    except LookupError:
        nltk.download('stopwords')

ensure_resources_downloaded()

# Streamlit ç•Œé¢
st.title("ğŸ§ AI vs Human Text Classification")
st.write("è¯·é€‰æ‹©æ¨¡å‹ï¼Œè¾“å…¥æ–‡æœ¬ï¼Œåˆ¤æ–­æ˜¯å¦ä¸º AI ç”Ÿæˆï¼Œå¹¶ä½¿ç”¨ DeepSeek æä¾›ä¾æ®å’Œé«˜äº®")

# **è®¾å¤‡è®¾ç½®**
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# **åŠ è½½ BERT æ¨¡å‹**
bert_model_path = "./saved_model"
bert_model = BertForSequenceClassification.from_pretrained(bert_model_path)
bert_tokenizer = BertTokenizer.from_pretrained(bert_model_path)
bert_model.to(device)
bert_model.eval()

# **åŠ è½½ RoBERTa æ¨¡å‹**
roberta_model_path = "./roberta_model"
roberta_model = RobertaForSequenceClassification.from_pretrained(roberta_model_path)
roberta_tokenizer = RobertaTokenizer.from_pretrained(roberta_model_path)
roberta_model.to(device)
roberta_model.eval()

# **åŠ è½½ LSTM æ¨¡å‹**
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate):
        super(LSTMClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True, dropout=dropout_rate)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        x = self.embedding(x)
        lstm_out, _ = self.lstm(x)
        lstm_out = lstm_out[:, -1, :]
        x = self.fc(self.dropout(lstm_out))
        return x

lstm_vocab_size = 40157
lstm_embedding_dim = 100
lstm_hidden_dim = 128
lstm_output_dim = 2
lstm_dropout_rate = 0.3

lstm_model = LSTMClassifier(lstm_vocab_size + 1, lstm_embedding_dim, lstm_hidden_dim, lstm_output_dim, lstm_dropout_rate)
lstm_model.load_state_dict(torch.load('./lstm_model.pth', map_location=device))
lstm_model.to(device)
lstm_model.eval()

# åŠ è½½ LSTM è¯æ±‡è¡¨
checkpoint = torch.load('./processed_data.pt', map_location=device)
word_to_idx = checkpoint['word_to_idx']


def load_model(model_path):
    with open(model_path, 'rb') as f:
        model = pickle.load(f)
    return model
# åŠ è½½æ‰€æœ‰æ¨¡å‹
logistic_regression_model = load_model('logistic_regression_pipeline.pkl')
naive_bayes_model = load_model('naive_bayes_pipeline.pkl')
xgboost_model = load_model('xgboost_pipeline.pkl')

# **æ–‡æœ¬é¢„å¤„ç†**
def remove_punc(text):
    return ''.join([char for char in text if char not in punctuation])

def remove_stop(text):
    stops = set(stopwords.words('english'))
    return " ".join([word for word in text.split() if word.lower() not in stops])

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)
    text = re.sub(r'<.*?>', '', text)
    text = remove_punc(text)
    text = remove_stop(text)
    return word_tokenize(text)

# **BERT / RoBERTa é¢„å¤„ç†**
def transformer_preprocess(text, tokenizer, max_length=256):
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=max_length,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )
    return encoding['input_ids'].to(device), encoding['attention_mask'].to(device)

# **LSTM é¢„å¤„ç†**
def lstm_preprocess(text, word_to_idx, max_len=200):
    tokens = preprocess_text(text)
    numerical = [word_to_idx.get(word, 0) for word in tokens]  # OOV è¯æ›¿æ¢ä¸º 0
    if len(numerical) > max_len:
        numerical = numerical[:max_len]
    else:
        numerical += [0] * (max_len - len(numerical))
    return torch.tensor([numerical], dtype=torch.long).to(device)

#xgboostç­‰æ¨¡å‹
def predict_with_model(model, text):
    prediction = model.predict([text])
    probabilities = model.predict_proba([text])
    predicted_class = prediction[0]
    if predicted_class == 1:
        predicted_class = "AI ç”Ÿæˆ"
    else:
        predicted_class = "äººç±»æ’°å†™"
    confidence = np.max(probabilities)  # ç½®ä¿¡åº¦æ˜¯æœ€é«˜æ¦‚ç‡
    return predicted_class, confidence

def predict(text, model_name):
    if model_name =="é€»è¾‘å›å½’":
        logistic_regression_prediction, logistic_regression_confidence = predict_with_model(logistic_regression_model,text)
        return logistic_regression_prediction, logistic_regression_confidence
    elif model_name == "æœ´ç´ è´å¶æ–¯":
        naive_bayes_prediction, naive_bayes_confidence = predict_with_model(naive_bayes_model,text)
        return naive_bayes_prediction,naive_bayes_confidence
    elif model_name == "XGBoost":
        xgboost_prediction, xgboost_confidence = predict_with_model(xgboost_model,text)
        return xgboost_prediction,xgboost_confidence
    elif model_name == "BERT":
        input_ids, attention_mask = transformer_preprocess(text, bert_tokenizer)
        model = bert_model
    elif model_name == "RoBERTa":
        input_ids, attention_mask = transformer_preprocess(text, roberta_tokenizer)
        model = roberta_model
    elif model_name == "LSTM":
        input_tensor = lstm_preprocess(text, word_to_idx)
        with torch.no_grad():
            outputs = lstm_model(input_tensor)
            probabilities = F.softmax(outputs, dim=1)
            predicted_class = torch.argmax(probabilities, dim=1).item()
            confidence = probabilities[0, predicted_class].item()
            return "AI ç”Ÿæˆ" if predicted_class == 1 else "äººç±»æ’°å†™", confidence

    # BERT / RoBERTa ç»Ÿä¸€æ¨ç†
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
        probabilities = F.softmax(outputs.logits, dim=1)
        predicted_class = torch.argmax(probabilities, dim=1).item()
        confidence = probabilities[0, predicted_class].item()

    return "AI ç”Ÿæˆ" if predicted_class == 1 else "äººç±»æ’°å†™", confidence

# -------------------- DeepSeek æ¥å£ --------------------
client = OpenAI(
    api_key="",
    base_url="https://api.deepseek.com"
)

def deepseek_detect(text):
    prompt = (
        "è¯·åˆ¤æ–­ä¸‹é¢çš„æ–‡æœ¬ä¸­å“ªäº›éƒ¨åˆ†å¯èƒ½æ˜¯ç”±AIç”Ÿæˆçš„ï¼Œå¹¶ç”¨[AI]å’Œ[/AI]æ ‡è®°åŒ…è£¹ï¼ŒåŒæ—¶ç»™å‡ºç†ç”±ï¼š\n\n"
        f"{text}\n\n"
        "è¯·è¿”å›æ ¼å¼ï¼š[é«˜äº®æ–‡æœ¬]\nè§£é‡Šï¼šxxxx"
    )

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt},
            ],
            stream=False
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"è°ƒç”¨å¤±è´¥ï¼š{e}"

def highlight_ai_text(deepseek_response):
    explanation = ""
    if "è§£é‡Šï¼š" in deepseek_response:
        parts = deepseek_response.split("è§£é‡Šï¼š")
        text_part = parts[0].strip()
        explanation = "è§£é‡Šï¼š" + parts[1].strip()
    else:
        text_part = deepseek_response

    highlighted = text_part.replace("[AI]", "<span style='background-color:#ffdddd'>") \
                           .replace("[/AI]", "</span>")
    return highlighted, explanation



# **Streamlit ç•Œé¢äº¤äº’**
model_choice = st.selectbox("é€‰æ‹©æ¨¡å‹", ["BERT", "RoBERTa", "LSTM", "é€»è¾‘å›å½’", "æœ´ç´ è´å¶æ–¯", "XGBoost"])
user_input = st.text_area("è¾“å…¥æ–‡æœ¬", "è¯·è¾“å…¥è¦åˆ†ç±»çš„æ–‡æœ¬...")

if st.button('åˆ†ç±»æ–‡æœ¬'):
    if user_input:
        result, confidence = predict(user_input, model_choice)
        st.subheader(f"åˆ†ç±»ç»“æœ: {result}")
        st.write(f"ç½®ä¿¡åº¦: {confidence:.4f}")
    else:
        st.error("è¯·è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼")
if st.button("ä½¿ç”¨ DeepSeek æ£€æµ‹å¹¶è§£é‡Š"):
    if user_input.strip():
        with st.spinner("DeepSeek æ£€æµ‹ä¸­..."):
            deepseek_result = deepseek_detect(user_input)
            highlighted_text, explanation = highlight_ai_text(deepseek_result)

        st.markdown("#### ğŸ•µï¸ DeepSeek æ£€æµ‹ç»“æœï¼š")
        st.markdown(highlighted_text, unsafe_allow_html=True)
        st.markdown(f"**{explanation}**")
    else:
        st.error("è¯·è¾“å…¥æ–‡æœ¬ä»¥ä½¿ç”¨ DeepSeek æ£€æµ‹ï¼")
